{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from struct import *\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate the values to positive\n",
    "translate = True\n",
    "\n",
    "#Neural Network\n",
    "n_hidden = 60 # neurons hidden layer\n",
    "learning_rate = 0.001 \n",
    "n_epochs = 10000 #number of epochs\n",
    "batch_size = 500 #size of the mini batch\n",
    "dropout_prob = 0.5\n",
    "\n",
    "n_training = 50000 #size of the training set\n",
    "n_test = 10000 #size of the test set\n",
    "beta = 0.01 # value for regularization\n",
    "\n",
    "activation_output = True # Activation function at the end\n",
    "dropout_bool = False # Drop out ?\n",
    "regularization = False # Regularization of the weights\n",
    "weight_saver = True # Save the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training set / test set\n",
    "The whole set is split into the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = np.load('output_data.npy')\n",
    "if translate:\n",
    "    output_data = output_data + 2 # On only keep positive values for the ReLu at the end of the network !! TO BE ADAPTED !!\n",
    "input_data = np.load('input_data.npy')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_data, test_size=0.33, random_state=42)\n",
    "\n",
    "x_train = X_train[:n_training,:]\n",
    "y_train = Y_train[:n_training,:]\n",
    "x_test = X_test[:n_test,:]\n",
    "y_test = Y_test[: n_test,:]\n",
    "\n",
    "N = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Tensors\n",
    "The network is a fully connected neural network with only one hidden layer composed of 40 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"place_holder\"):\n",
    "    x = tf.placeholder(tf.float32, (None, 144))\n",
    "    y = tf.placeholder(tf.float32, (None, 2))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"hidden_layer\", reuse = tf.AUTO_REUSE):\n",
    "    W_hidden = tf.get_variable(\"W_hidden\",shape = [144, n_hidden], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_hidden = tf.get_variable(\"b_hidden\",shape = [n_hidden,], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    x_hidden = tf.nn.relu(tf.matmul(x, W_hidden) + b_hidden) # ACTIVATION FUNCTION \n",
    "    \n",
    "    if dropout_bool:\n",
    "        x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "\n",
    "with tf.variable_scope(\"output\", reuse = tf.AUTO_REUSE):\n",
    "    W_output = tf.get_variable(\"W_output\",shape = [n_hidden, 2], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_output = tf.get_variable(\"b_output\",shape = [2,], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    output = tf.matmul(x_hidden, W_output) + b_output\n",
    "    \n",
    "    if activation_output:\n",
    "        output = tf.nn.relu(tf.matmul(x_hidden, W_output) + b_output) # ACTIVATION FUNCTION \n",
    "    \n",
    "with tf.variable_scope(\"loss\", reuse = tf.AUTO_REUSE):\n",
    "    l = tf.reduce_mean(tf.square(output - y))\n",
    "    \n",
    "    if regularization:\n",
    "        l = tf.reduce_mean(l + beta * tf.nn.l2_loss(W_hidden) + beta * tf.nn.l2_loss(W_output) )\n",
    "        \n",
    "with tf.variable_scope(\"optim\", reuse = tf.AUTO_REUSE):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "    \n",
    "if weight_saver:\n",
    "    with tf.variable_scope(\"saver\", reuse = tf.AUTO_REUSE):\n",
    "        saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "loss_value = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    step = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "        while pos < N:\n",
    "            batch_x = x_train[pos:pos + batch_size, :]\n",
    "            batch_y = y_train[pos:pos + batch_size, :]\n",
    "            if dropout_bool:\n",
    "                feed_dict = {x:batch_x, y: batch_y, keep_prob:dropout_prob}\n",
    "            else:\n",
    "                feed_dict = {x:batch_x, y: batch_y}\n",
    "            train, loss = sess.run([train_op,l], feed_dict=feed_dict)\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "            loss_value.append(loss)\n",
    "        print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "        \n",
    "    t2 = time.time()\n",
    "    print ('The training lasts %f' %(t2 - t1))\n",
    "    \n",
    "    \n",
    "    #Save Weights\n",
    "    if weight_saver:\n",
    "        save_path = saver.save(sess,\"tmp/model.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "    #Make predictions\n",
    "    if dropout_bool:\n",
    "        y_pred = sess.run(output, feed_dict = {x:x_test, keep_prob:1.0})\n",
    "    else:\n",
    "        y_pred = sess.run(output, feed_dict = {x:x_test})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    RMSE = tf.sqrt(tf.losses.mean_squared_error(y_pred, y_test))\n",
    "    RMSE_value = sess.run(RMSE)\n",
    "    mean_error_value =  mean_absolute_error(y_test, y_pred)\n",
    "    print(\"RMSE error : {}\".format(RMSE_value))\n",
    "    print(\"Average of absolute differences : {}\".format(mean_error_value))\n",
    "    print(\"Average of absolute differences : {}\".format(mean_error_value))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
